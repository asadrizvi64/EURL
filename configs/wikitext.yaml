model:
  vocab_size: 30000
  d_model: 512
  nhead: 8
  num_layers: 6
  dim_feedforward: 2048
  dropout: 0.1
  sparse_rank: 64
  kappa: 0.5
  c_const: 2.0

training:
  batch_size: 16
  learning_rate: 0.0001
  epochs: 10
  warmup_steps: 4000
  max_seq_length: 512